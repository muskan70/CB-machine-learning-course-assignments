{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#clean input file\n",
    "def getCleanDocument(inputFile,outputFile):\n",
    "    out=open(outputFile,'w',encoding=\"utf8\")\n",
    "    with open(inputFile,encoding=\"utf8\") as f:\n",
    "        reviews=f.readlines()\n",
    "        \n",
    "    for review in reviews:\n",
    "        cleaned_review=getCleanText(review)\n",
    "        print(cleaned_review,file=out)\n",
    "    \n",
    "    out.close()\n",
    "    \n",
    "#clean a review\n",
    "def getCleanText(text):\n",
    "    text=text.lower()\n",
    "    text=text.replace(\"<br /><br />\",\" \")\n",
    "    \n",
    "    #Init objects\n",
    "    tokenizer=RegexpTokenizer(r'[a-z]+')\n",
    "    l=WordNetLemmatizer()\n",
    "    en_stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "                  \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n",
    "                  'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
    "                  'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
    "                  'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was',\n",
    "                  'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "                  'does', 'did', 'doing', 'a', 'an', 'the', 'until', 'while', 'of', 'at',\n",
    "                  'by', 'for', 'with', 'about', 'into', 'through', 'during', 'before',\n",
    "                  'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "                  'on', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                  'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "                  'most', 'other', 'some', 'such', 'own', 'same', 'so', 'than', 'too', \n",
    "                  'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now',\n",
    "                  'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'ma', 'shan', \"shan't\"]\n",
    "    \n",
    "    #Tokenize\n",
    "    tokens=tokenizer.tokenize(text)\n",
    "    new_tokens=[token for token in tokens if token not in en_stopwords]\n",
    "    stemmed_tokens=[l.lemmatize(token) for token in new_tokens]\n",
    "    cleaned_text=' '.join(stemmed_tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get train dataset\n",
    "getCleanDocument(\"../../Datasets/IMDB/imdb_trainX.txt\",\"Xtrain.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test dataset\n",
    "getCleanDocument(\"../../Datasets/IMDB/imdb_testX.txt\",\"Xtest.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "with open(\"Xtrain.txt\",'r') as f:\n",
    "    X_train=f.readlines()\n",
    "print(type(X_train)) \n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "with open(\"Xtest.txt\",'r') as f:\n",
    "    X_test=f.readlines()\n",
    "print(type(X_test)) \n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../Datasets/IMDB/imdb_trainY.txt\",'r') as f:\n",
    "    Y_train=f.readlines()\n",
    "print(type(Y_train)) \n",
    "Y_train=[int(i) for i in Y_train]\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../Datasets/IMDB/imdb_testY.txt\",'r') as f:\n",
    "    Y_test=f.readlines()\n",
    "Y_test=[int(i) for i in Y_test]    \n",
    "print(type(Y_test))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved movie since and saw opening day touching and beautiful strongly recommend seeing movie watch family far mpaa rating pg thematic element prolonged scene disastor nudity sexuality and language\n",
      "\n",
      "10\n",
      "not really sure make movie weird artsy not kind movie watch because compelling plot or character like kind movie stop watching because horrifically fascinating thing happening screen although first time wife watched couldn make way disturbing run bit long but nonetheless worthwhile viewing interested dark movie\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(Y_train[0])\n",
    "print(X_test[0])\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.array(X_train)\n",
    "Y_train=np.array(Y_train)\n",
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "X_test=np.array(X_test)\n",
    "Y_test=np.array(Y_test)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 65525)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv=TfidfVectorizer()\n",
    "x_vec=cv.fit_transform(X_train)\n",
    "print(x_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superbly developed character lot funny situation full spirit absurdness and serbian mentality movie great comedy enjoyable interesting unpredictable best point film character humor story and dialog humor inner development rare serbian movie consequence characterization well motivated spontaneous and cogent also sharp intelligent and lucid movie unfortunately constructed humor devise joke and put character mouth or ordinary situation comedy burlesque farce art immortality incorporated movie little masterpiece hardly reachable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1515])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27450\n",
      "<class 'dict'>\n",
      "faultline\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_[\"humor\"])\n",
    "print(type(cv.vocabulary_))\n",
    "print(cv.get_feature_names()[20283])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Train Model using sklearn MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 65525)\n"
     ]
    }
   ],
   "source": [
    "xt_vec=cv.transform(X_test)\n",
    "print(xt_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91688\n",
      "0.32452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb=MultinomialNB(alpha=0.001)\n",
    "mnb.fit(x_vec,Y_train)\n",
    "print(mnb.score(x_vec,Y_train))\n",
    "print(mnb.score(xt_vec,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Multinomial Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "Y_train_list=pd.Series(Y_train).value_counts(sort=False)\n",
    "ALL=Y_train_list.sum()\n",
    "print(type(Y_train_list))\n",
    "print(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.204, 2: 0.09136, 3: 0.0968, 4: 0.10784, 7: 0.09984, 8: 0.12036, 9: 0.09052, 10: 0.18928}\n"
     ]
    }
   ],
   "source": [
    "prior_probs={}\n",
    "for label,count in Y_train_list.items():\n",
    "    prior_probs[label]=count/ALL\n",
    "print(prior_probs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have document ID ,word Id and count of tfidx with non zero value\n",
    "docIdx,wordIdx=x_vec.nonzero()\n",
    "count=x_vec.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2533270,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(type(docIdx))\n",
    "print(docIdx.shape)\n",
    "print(docIdx[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2533270,)\n",
      "[32568 51588 40234 15544 50487 45267 17852 57928 43194 46671 38413 20104\n",
      " 20017 63196 51113 47100 55570  4794 58939 13759 40972 50334  1945 52676\n",
      " 38368 34247 23078  1495 51128 58056 47103 26431 24603 25854 16772 50746\n",
      " 38713 33721 51010 17517 56488 14359 11930 63463 19762  9109 21515 35547\n",
      " 38737 20311 43421 31843 55986 57617 16675 51122 63274 37005 44058 58140\n",
      " 18311 33258 39519 54875 24481  5750 59166 55440 28923 10163 14605 56701\n",
      " 57614 27828  4753  3111   684 55418 14824 26252  2263 46101 38068 37830\n",
      " 29242 35374 16360 23205 57008  2363 63975  7490 64507 40875 64311 35010\n",
      "  3432 56929 46097 23820]\n"
     ]
    }
   ],
   "source": [
    "print(type(wordIdx))\n",
    "print(wordIdx.shape)\n",
    "print(wordIdx[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2533270,)\n",
      "[0.18986371 0.24212233 0.19032227 0.40507677 0.0906802  0.29179566\n",
      " 0.16442368 0.30261658 0.24453083 0.16682429 0.29623759 0.12737802\n",
      " 0.13217568 0.09727404 0.13900058 0.14564698 0.22329518 0.14080475\n",
      " 0.19784931 0.11836062 0.16806098 0.12436754 0.12043495 0.12826952\n",
      " 0.11117379 0.15522731 0.03762899 0.02968507 0.02331223 0.02266725\n",
      " 0.04527018 0.03803986 0.07651778 0.04216138 0.06684505 0.03862413\n",
      " 0.04146245 0.04913631 0.05080626 0.04628782 0.0540485  0.0519067\n",
      " 0.06261511 0.02010703 0.03906417 0.06100811 0.04985506 0.05293692\n",
      " 0.0292413  0.04896511 0.04369516 0.04633013 0.05626634 0.05973257\n",
      " 0.04922313 0.03723575 0.02148672 0.04945875 0.02798303 0.02692777\n",
      " 0.04722512 0.03106947 0.02386085 0.03046108 0.06166263 0.05732949\n",
      " 0.07319381 0.06883925 0.05356885 0.05175211 0.06274144 0.05740022\n",
      " 0.06224622 0.06773723 0.05684911 0.04338252 0.06313126 0.07016902\n",
      " 0.06382026 0.0382744  0.04701217 0.06664807 0.04792406 0.0937302\n",
      " 0.02979597 0.02377734 0.02969196 0.01926227 0.02487832 0.03003618\n",
      " 0.0495487  0.07994972 0.01944625 0.0284452  0.03785311 0.05732051\n",
      " 0.0497006  0.06441139 0.0606976  0.05167578]\n"
     ]
    }
   ],
   "source": [
    "print(type(count))\n",
    "print(count.shape)\n",
    "print(count[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2533270\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "classIdx=[]\n",
    "for i in docIdx:\n",
    "    classIdx.append(Y_train[i])\n",
    "print(len(classIdx))\n",
    "print(classIdx[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2533270, 4)\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "df['docIdx']=docIdx\n",
    "df['wordIdx']=wordIdx\n",
    "df['count']=count\n",
    "df['classIdx']=np.array(classIdx)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>wordIdx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>65515</th>\n",
       "      <th>65516</th>\n",
       "      <th>65517</th>\n",
       "      <th>65518</th>\n",
       "      <th>65519</th>\n",
       "      <th>65520</th>\n",
       "      <th>65521</th>\n",
       "      <th>65522</th>\n",
       "      <th>65523</th>\n",
       "      <th>65524</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classIdx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.126451e-06</td>\n",
       "      <td>2.408257e-06</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>3.599132e-07</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>9.451691e-07</td>\n",
       "      <td>3.599132e-07</td>\n",
       "      <td>4.257889e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.702705e-06</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>5.930581e-07</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>9.415102e-09</td>\n",
       "      <td>1.481174e-06</td>\n",
       "      <td>1.481174e-06</td>\n",
       "      <td>1.481174e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "      <td>1.185038e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.333950e-06</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>2.982074e-06</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>2.169683e-06</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.584023e-06</td>\n",
       "      <td>4.607615e-06</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "      <td>1.162248e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>3.549597e-06</td>\n",
       "      <td>1.772279e-06</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.331588e-06</td>\n",
       "      <td>2.265320e-06</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>2.405236e-06</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "      <td>1.127529e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "      <td>1.148263e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.953427e-06</td>\n",
       "      <td>2.088005e-06</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "      <td>1.097868e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.075838e-06</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>2.612285e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "      <td>1.185710e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>2.161605e-06</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.613449e-07</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "      <td>9.810896e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 65525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "wordIdx          0             1             2             3      \\\n",
       "classIdx                                                           \n",
       "1         3.126451e-06  2.408257e-06  9.415102e-09  3.599132e-07   \n",
       "2         1.185038e-08  1.185038e-08  1.185038e-08  1.185038e-08   \n",
       "3         2.333950e-06  1.162248e-08  1.162248e-08  1.162248e-08   \n",
       "4         1.127529e-08  3.549597e-06  1.772279e-06  1.127529e-08   \n",
       "7         1.148263e-08  1.148263e-08  1.148263e-08  1.148263e-08   \n",
       "8         1.953427e-06  2.088005e-06  1.097868e-08  1.097868e-08   \n",
       "9         1.185710e-08  1.185710e-08  1.185710e-08  1.185710e-08   \n",
       "10        9.810896e-09  2.161605e-06  9.810896e-09  9.810896e-09   \n",
       "\n",
       "wordIdx          4             5             6             7      \\\n",
       "classIdx                                                           \n",
       "1         9.415102e-09  9.415102e-09  9.415102e-09  9.451691e-07   \n",
       "2         1.185038e-08  1.185038e-08  1.185038e-08  1.185038e-08   \n",
       "3         1.162248e-08  2.982074e-06  1.162248e-08  1.162248e-08   \n",
       "4         1.127529e-08  1.127529e-08  1.127529e-08  1.127529e-08   \n",
       "7         1.148263e-08  1.148263e-08  1.148263e-08  1.148263e-08   \n",
       "8         1.097868e-08  1.097868e-08  1.097868e-08  1.097868e-08   \n",
       "9         1.075838e-06  1.185710e-08  1.185710e-08  1.185710e-08   \n",
       "10        9.810896e-09  9.810896e-09  9.613449e-07  9.810896e-09   \n",
       "\n",
       "wordIdx          8             9      ...         65515         65516  \\\n",
       "classIdx                              ...                               \n",
       "1         3.599132e-07  4.257889e-06  ...  1.702705e-06  9.415102e-09   \n",
       "2         1.185038e-08  1.185038e-08  ...  1.185038e-08  1.185038e-08   \n",
       "3         1.162248e-08  1.162248e-08  ...  1.162248e-08  2.169683e-06   \n",
       "4         1.127529e-08  1.127529e-08  ...  1.127529e-08  1.331588e-06   \n",
       "7         1.148263e-08  1.148263e-08  ...  1.148263e-08  1.148263e-08   \n",
       "8         1.097868e-08  1.097868e-08  ...  1.097868e-08  1.097868e-08   \n",
       "9         1.185710e-08  2.612285e-06  ...  1.185710e-08  1.185710e-08   \n",
       "10        9.810896e-09  9.810896e-09  ...  9.810896e-09  9.810896e-09   \n",
       "\n",
       "wordIdx          65517         65518         65519         65520  \\\n",
       "classIdx                                                           \n",
       "1         9.415102e-09  5.930581e-07  9.415102e-09  9.415102e-09   \n",
       "2         1.185038e-08  1.185038e-08  1.185038e-08  1.185038e-08   \n",
       "3         1.162248e-08  1.162248e-08  1.584023e-06  4.607615e-06   \n",
       "4         2.265320e-06  1.127529e-08  1.127529e-08  1.127529e-08   \n",
       "7         1.148263e-08  1.148263e-08  1.148263e-08  1.148263e-08   \n",
       "8         1.097868e-08  1.097868e-08  1.097868e-08  1.097868e-08   \n",
       "9         1.185710e-08  1.185710e-08  1.185710e-08  1.185710e-08   \n",
       "10        9.810896e-09  9.810896e-09  9.810896e-09  9.810896e-09   \n",
       "\n",
       "wordIdx          65521         65522         65523         65524  \n",
       "classIdx                                                          \n",
       "1         9.415102e-09  1.481174e-06  1.481174e-06  1.481174e-06  \n",
       "2         1.185038e-08  1.185038e-08  1.185038e-08  1.185038e-08  \n",
       "3         1.162248e-08  1.162248e-08  1.162248e-08  1.162248e-08  \n",
       "4         2.405236e-06  1.127529e-08  1.127529e-08  1.127529e-08  \n",
       "7         1.148263e-08  1.148263e-08  1.148263e-08  1.148263e-08  \n",
       "8         1.097868e-08  1.097868e-08  1.097868e-08  1.097868e-08  \n",
       "9         1.185710e-08  1.185710e-08  1.185710e-08  1.185710e-08  \n",
       "10        9.810896e-09  9.810896e-09  9.810896e-09  9.810896e-09  \n",
       "\n",
       "[8 rows x 65525 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alpha value for smoothing\n",
    "a = 0.001\n",
    "vocab_size=len(cv.vocabulary_)\n",
    "\n",
    "#Calculate probability of each word based on class\n",
    "pb_ij = df.groupby(['classIdx','wordIdx'])\n",
    "pb_j = df.groupby(['classIdx'])\n",
    "Pr =  (pb_ij['count'].sum() + a) / (pb_j['count'].sum() + vocab_size)    \n",
    "\n",
    "#Unstack series\n",
    "Pr = Pr.unstack()\n",
    "\n",
    "#Replace NaN or columns with 0 as word count with a/(count+|V|+1)\n",
    "\n",
    "for c in np.unique(Y_train):\n",
    "    Pr.loc[c,:] = Pr.loc[c,:].fillna(a/(pb_j['count'].sum()[c] + vocab_size))\n",
    "\n",
    "#Convert to dictionary for greater speed\n",
    "Pr_dict = Pr.to_dict()\n",
    "\n",
    "Pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate IDF \n",
    "tot = len(df['docIdx'].unique()) \n",
    "pb_ij = df.groupby(['wordIdx']) \n",
    "IDF = np.log(tot/pb_ij['docIdx'].count()) \n",
    "IDF_dict = IDF.to_dict()\n",
    "#print(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB(df, smooth = False, IDF = False):\n",
    "    '''\n",
    "    Multinomial Naive Bayes classifier\n",
    "    :param df [Pandas Dataframe]: Dataframe of data\n",
    "    :param smooth [bool]: Apply Smoothing if True\n",
    "    :param IDF [bool]: Apply Inverse Document Frequency if True\n",
    "    :return predict [list]: Predicted class ID\n",
    "    '''\n",
    "    #Using dictionaries for greater speed\n",
    "    df_dict = df.to_dict()\n",
    "    new_dict = {}\n",
    "    prediction = []\n",
    "    print(len(df_dict['docIdx']))\n",
    "    #new_dict = {docIdx : {wordIdx: count},....}\n",
    "    for idx in range(len(df_dict['docIdx'])):\n",
    "        docIdx = df_dict['docIdx'][idx]\n",
    "        wordIdx = df_dict['wordIdx'][idx]\n",
    "        count = df_dict['count'][idx]\n",
    "        try: \n",
    "            new_dict[docIdx][wordIdx] = count \n",
    "        except:\n",
    "            new_dict[df_dict['docIdx'][idx]] = {}\n",
    "            new_dict[docIdx][wordIdx] = count\n",
    "\n",
    "    #Calculating the scores for each doc\n",
    "    for docIdx in range(len(new_dict)):\n",
    "        score_dict = {}\n",
    "        #Creating a probability row for each class\n",
    "        for classIdx in np.unique(Y_train):\n",
    "            score_dict[classIdx] = 1\n",
    "            #For each word:\n",
    "            for wordIdx in new_dict[docIdx]:\n",
    "                #Check for frequency smoothing\n",
    "                #log(1+f)*log(Pr(i|j))\n",
    "                if smooth: \n",
    "                    try:\n",
    "                        probability=Pr_dict[wordIdx][classIdx]         \n",
    "                        power = np.log(1+ new_dict[docIdx][wordIdx])     \n",
    "                        #Check for IDF\n",
    "                        if IDF:\n",
    "                            score_dict[classIdx]+=(\n",
    "                               power*np.log(\n",
    "                               probability*IDF_dict[wordIdx]))\n",
    "                        else:\n",
    "                            score_dict[classIdx]+=power*np.log(\n",
    "                                                   probability)\n",
    "                    except:\n",
    "                        #Missing V will have log(1+0)*log(a/16689)=0 \n",
    "                        score_dict[classIdx] += 0                        \n",
    "                #f*log(Pr(i|j))\n",
    "                else: \n",
    "                    try:\n",
    "                        probability = Pr_dict[wordIdx][classIdx]        \n",
    "                        power = new_dict[docIdx][wordIdx]               \n",
    "                        score_dict[classIdx]+=power*np.log(\n",
    "                                           probability) \n",
    "                        #Check for IDF\n",
    "                        if IDF:\n",
    "                            score_dict[classIdx]+= power*np.log(\n",
    "                                   probability*IDF_dict[wordIdx]) \n",
    "                    except:\n",
    "                        #Missing V will have 0*log(a/16689) = 0\n",
    "                        score_dict[classIdx] += 0      \n",
    "            #Multiply final with pi         \n",
    "            score_dict[classIdx] +=  np.log(prior_probs[classIdx])                          \n",
    "\n",
    "        #Get class with max probabilty for the given docIdx \n",
    "        max_score = max(score_dict, key=score_dict.get)\n",
    "        prediction.append(max_score)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2533270\n",
      "0.6404\n"
     ]
    }
   ],
   "source": [
    "y_pred=MNB(df)\n",
    "print(np.sum(Y_train==y_pred)/25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6404\n"
     ]
    }
   ],
   "source": [
    "val=0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]==Y_train[i]:\n",
    "        val+=1\n",
    "print(val/25000)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5090    0    0    0    0    0    0   10]\n",
      " [1599  516    1    7    2   10    0  149]\n",
      " [1370    0  843   10    1    7    0  189]\n",
      " [1049    0    0 1446    1    5    0  195]\n",
      " [ 478    0    0    2 1148   41    0  827]\n",
      " [ 381    0    0    2    2 1824    0  800]\n",
      " [ 339    0    0    1    6   71  594 1252]\n",
      " [ 179    0    0    1    1    2    0 4549]]\n"
     ]
    }
   ],
   "source": [
    "#Generate Confusion Matrix of train_data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix=confusion_matrix(Y_train,y_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have document ID ,word Id and count of tfidx with non zero value\n",
    "docIdx,wordIdx=xt_vec.nonzero()\n",
    "count=xt_vec.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2443273\n"
     ]
    }
   ],
   "source": [
    "classIdx=[]\n",
    "for i in docIdx:\n",
    "    classIdx.append(Y_test[i])\n",
    "print(len(classIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2443273, 4)\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "df['docIdx']=docIdx\n",
    "df['wordIdx']=wordIdx\n",
    "df['count']=count\n",
    "df['classIdx']=np.array(classIdx)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2443273\n",
      "0.32336\n"
     ]
    }
   ],
   "source": [
    "yt_pred=MNB(df)\n",
    "print(np.sum(Y_test==yt_pred)/25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4540    3    8   36    6   21    0  408]\n",
      " [1965    4   10   32    8   17    0  266]\n",
      " [2001    4   24   42   10   50    1  409]\n",
      " [1838    3    8   80   16   75    0  615]\n",
      " [ 991    1    6   37   42  111    2 1117]\n",
      " [ 988    2   11   40   23  164    4 1618]\n",
      " [ 780    2    6   17   12  114    5 1408]\n",
      " [1565    5   10   27   17  142    8 3225]]\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix=confusion_matrix(Y_test,yt_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2443273\n"
     ]
    }
   ],
   "source": [
    "yt_pred1=MNB(df,True,False)\n",
    "yt_pred2=MNB(df,False,True)\n",
    "yt_pred3=MNB(df,True,True)\n",
    "print(np.sum(Y_test==yt_pred1)/25000)\n",
    "print(np.sum(Y_test==yt_pred2)/25000)\n",
    "print(np.sum(Y_test==yt_pred3)/25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
